{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssX_map8c6mx"
   },
   "source": [
    "# 🐫 CAMEL VLM Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "AkvGo5umOwxM"
   },
   "outputs": [],
   "source": [
    "# !pip install camel-ai[all]==\"0.2.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-8_5_srcpC_",
    "outputId": "ac4ef9cb-3d34-4b62-ed72-54392d831d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your API key: ··········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Prompt for the API key securely\n",
    "openai_api_key = getpass('Enter your API key: ')\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J97N3GtSexwu"
   },
   "outputs": [],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.configs import ChatGPTConfig\n",
    "from camel.messages import BaseMessage\n",
    "from camel.types import ModelType, ModelPlatformType\n",
    "from camel.types.enums import RoleType\n",
    "from camel.models import ModelFactory\n",
    "\n",
    "\n",
    "sys_msg = BaseMessage.make_assistant_message(\n",
    "    role_name=\"Assistant\",\n",
    "    content=\"You're a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Set model\n",
    "model=ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI,\n",
    "    model_type=ModelType.GPT_4O,\n",
    "    model_config_dict=ChatGPTConfig(temperature=0.6).as_dict(),\n",
    ")\n",
    "\n",
    "# Set agent\n",
    "vlm_agent = ChatAgent(\n",
    "    sys_msg,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQIJBZh_YXRY",
    "outputId": "6867694f-e0f4-4974-9105-69ff1e9b4806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image contains the logo for \"CAMEL-AI,\" featuring a stylized purple camel next to the text \"CAMEL-AI\" in purple.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# URL of the image\n",
    "url = \"https://raw.githubusercontent.com/camel-ai/camel/master/misc/logo_light.png\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "user_msg = BaseMessage.make_user_message(\n",
    "    role_name=\"User\", content=\"\"\"what's in the image?\"\"\", image_list=[img]\n",
    ")\n",
    "\n",
    "# Get response information\n",
    "response = vlm_agent.step(user_msg)\n",
    "print(response.msgs[0].content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
